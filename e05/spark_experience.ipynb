{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This practical uses the file `flight_info.csv`, which you can download from [this link](https://www.dropbox.com/scl/fi/jsbatbyh5hl8q187986v1/flight_info.csv?rlkey=8ky753wksm5e6zbxdhvcoz2c9&dl=1).\n",
    "\n",
    "**Important**: You must use Apache Spark for all tasks, even though this can technically run on a single machine. The goal is to practice using Spark in a way that simulates a distributed computing environment, so avoid any approach that wouldn't run on a single machine if the dataset was substantially larger. \n",
    "\n",
    "1. Read the file into a Spark DataFrame named `df`.\n",
    "   * Ensure the schema is inferred automatically, and indicate that the file includes a header row.\n",
    "\n",
    "2. Use any Spark method of your choice to display the first 10 rows of the DataFrame.\n",
    "\n",
    "3. How many unique carriers are listed in the file?\n",
    "\n",
    "4. How many flights are associated with each carrier?\n",
    "\n",
    "5. Which hour of the day has the most flight take offs?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the file into a Spark DataFrame named `df`.\n",
    "   * Ensure the schema is inferred automatically, and indicate that the file includes a header row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/03 12:37:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"flight_info\").getOrCreate()\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./flight_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use any Spark method of your choice to display the first 10 rows of the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/03 12:37:30 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , DayOfWeek, UniqueCarrier, FlightNum, Origin, Dest, CRSDepTime, DepTime, TaxiOut, WheelsOff, WheelsOn, TaxiIn, CRSArrTime, ArrTime, Cancelled, CancellationCode, Distance, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay\n",
      " Schema: _c0, DayOfWeek, UniqueCarrier, FlightNum, Origin, Dest, CRSDepTime, DepTime, TaxiOut, WheelsOff, WheelsOn, TaxiIn, CRSArrTime, ArrTime, Cancelled, CancellationCode, Distance, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/sebastiansole/Documents/NTNU/9sem/bigdata/ics-438-assignments/e05/flight_info.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------+---------+------+----+----------+-------+-------+---------+--------+------+----------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|_c0|DayOfWeek|UniqueCarrier|FlightNum|Origin|Dest|CRSDepTime|DepTime|TaxiOut|WheelsOff|WheelsOn|TaxiIn|CRSArrTime|ArrTime|Cancelled|CancellationCode|Distance|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+---+---------+-------------+---------+------+----+----------+-------+-------+---------+--------+------+----------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|  0|        2|           AA|      494|   CLT| PHX|      1619| 1616.0|   17.0|   1633.0|  1837.0|   5.0|      1856| 1842.0|      0.0|            NULL|  1773.0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|  1|        3|           AA|      494|   CLT| PHX|      1619| 1614.0|   13.0|   1627.0|  1815.0|   6.0|      1856| 1821.0|      0.0|            NULL|  1773.0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|  2|        4|           AA|      494|   CLT| PHX|      1619| 1611.0|   17.0|   1628.0|  1824.0|   2.0|      1856| 1826.0|      0.0|            NULL|  1773.0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|  3|        5|           AA|      494|   CLT| PHX|      1619| 1656.0|   18.0|   1714.0|  1926.0|   3.0|      1856| 1929.0|      0.0|            NULL|  1773.0|        33.0|         0.0|     0.0|          0.0|              0.0|\n",
      "|  4|        6|           AA|      494|   CLT| PHX|      1619| 1632.0|   17.0|   1649.0|  1854.0|   4.0|      1856| 1858.0|      0.0|            NULL|  1773.0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|  5|        7|           AA|      494|   CLT| PHX|      1619| 1636.0|   27.0|   1703.0|  1917.0|   4.0|      1856| 1921.0|      0.0|            NULL|  1773.0|         0.0|         0.0|     8.0|          0.0|             17.0|\n",
      "|  6|        1|           AA|      494|   CLT| PHX|      1619| 1616.0|   20.0|   1636.0|  1900.0|   7.0|      1856| 1907.0|      0.0|            NULL|  1773.0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|  7|        2|           AA|      494|   CLT| PHX|      1619| 1619.0|   18.0|   1637.0|  1858.0|   6.0|      1856| 1904.0|      0.0|            NULL|  1773.0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|  8|        3|           AA|      494|   CLT| PHX|      1619| 1616.0|   23.0|   1639.0|  1901.0|   5.0|      1856| 1906.0|      0.0|            NULL|  1773.0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|  9|        4|           AA|      494|   CLT| PHX|      1619| 1618.0|   11.0|   1629.0|  1857.0|   5.0|      1856| 1902.0|      0.0|            NULL|  1773.0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "+---+---------+-------------+---------+------+----+----------+-------+-------+---------+--------+------+----------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How many unique carriers are listed in the file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==================================>                       (6 + 4) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|UniqueCarrier|\n",
      "+-------------+\n",
      "|           AA|\n",
      "|           EV|\n",
      "|           B6|\n",
      "|           DL|\n",
      "|           UA|\n",
      "|           NK|\n",
      "|           OO|\n",
      "|           F9|\n",
      "|           HA|\n",
      "|           WN|\n",
      "|           AS|\n",
      "|           VX|\n",
      "+-------------+\n",
      "\n",
      "Total number of unique carriers:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Testing without RDD\n",
    "unique_carriers = df.select(\"UniqueCarrier\").distinct().count()\n",
    "\n",
    "df.select(\"UniqueCarrier\").distinct().show()\n",
    "print(\"Total number of unique carriers: \", unique_carriers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a RDD\n",
    "df_rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/03 12:37:36 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , DayOfWeek, UniqueCarrier, FlightNum, Origin, Dest, CRSDepTime, DepTime, TaxiOut, WheelsOff, WheelsOn, TaxiIn, CRSArrTime, ArrTime, Cancelled, CancellationCode, Distance, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay\n",
      " Schema: _c0, DayOfWeek, UniqueCarrier, FlightNum, Origin, Dest, CRSDepTime, DepTime, TaxiOut, WheelsOff, WheelsOn, TaxiIn, CRSArrTime, ArrTime, Cancelled, CancellationCode, Distance, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/sebastiansole/Documents/NTNU/9sem/bigdata/ics-438-assignments/e05/flight_info.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique carriers:  12\n",
      "[('DL', 69813), ('UA', 42403), ('AA', 73132), ('HA', 6276), ('B6', 24602), ('EV', 35037), ('AS', 14711), ('VX', 5782), ('F9', 7760), ('NK', 12570), ('OO', 50146), ('WN', 107785)]\n"
     ]
    }
   ],
   "source": [
    "# Testing with RDD\n",
    "unique_carriers_rdd = df_rdd.map(lambda row: (row['UniqueCarrier'], 1)).reduceByKey(lambda a, b: a + b)\n",
    "collect = unique_carriers_rdd.collect()\n",
    "unique_carriers_count = unique_carriers_rdd.count()\n",
    "\n",
    "print(\"Total number of unique carriers: \", unique_carriers_count)\n",
    "print(collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. How many flights were there from each carrier? (Hint: Use wheel off)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_takeoffs_rdd = df_rdd.filter(lambda flight: flight['WheelsOff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/03 12:37:41 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , DayOfWeek, UniqueCarrier, FlightNum, Origin, Dest, CRSDepTime, DepTime, TaxiOut, WheelsOff, WheelsOn, TaxiIn, CRSArrTime, ArrTime, Cancelled, CancellationCode, Distance, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay\n",
      " Schema: _c0, DayOfWeek, UniqueCarrier, FlightNum, Origin, Dest, CRSDepTime, DepTime, TaxiOut, WheelsOff, WheelsOn, TaxiIn, CRSArrTime, ArrTime, Cancelled, CancellationCode, Distance, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/sebastiansole/Documents/NTNU/9sem/bigdata/ics-438-assignments/e05/flight_info.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('DL', 69044),\n",
       " ('UA', 42174),\n",
       " ('AA', 72159),\n",
       " ('HA', 6258),\n",
       " ('B6', 24082),\n",
       " ('EV', 33890),\n",
       " ('AS', 14458),\n",
       " ('VX', 5645),\n",
       " ('F9', 7598),\n",
       " ('NK', 12133),\n",
       " ('OO', 48324),\n",
       " ('WN', 105479)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_takeoffs_rdd_mapped = flight_takeoffs_rdd.map(lambda flight: (flight['UniqueCarrier'], 1)).reduceByKey(lambda a, b: a + b)\n",
    "flight_takeoffs_rdd_mapped.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Which hour of the day has the most flight take offs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/03 12:37:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , DayOfWeek, UniqueCarrier, FlightNum, Origin, Dest, CRSDepTime, DepTime, TaxiOut, WheelsOff, WheelsOn, TaxiIn, CRSArrTime, ArrTime, Cancelled, CancellationCode, Distance, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay\n",
      " Schema: _c0, DayOfWeek, UniqueCarrier, FlightNum, Origin, Dest, CRSDepTime, DepTime, TaxiOut, WheelsOff, WheelsOn, TaxiIn, CRSArrTime, ArrTime, Cancelled, CancellationCode, Distance, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/sebastiansole/Documents/NTNU/9sem/bigdata/ics-438-assignments/e05/flight_info.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('0', 1045),\n",
       " ('1', 372),\n",
       " ('2', 215),\n",
       " ('3', 126),\n",
       " ('4', 86),\n",
       " ('5', 9224),\n",
       " ('6', 28945),\n",
       " ('7', 30262),\n",
       " ('8', 29952),\n",
       " ('9', 27649),\n",
       " ('10', 26924),\n",
       " ('11', 28343),\n",
       " ('12', 26958),\n",
       " ('13', 27496),\n",
       " ('14', 25611),\n",
       " ('15', 28252),\n",
       " ('16', 25795),\n",
       " ('17', 29886),\n",
       " ('18', 25178),\n",
       " ('19', 24025),\n",
       " ('20', 19877),\n",
       " ('21', 12279),\n",
       " ('22', 9264),\n",
       " ('23', 3480)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_times = flight_takeoffs_rdd.map(lambda flight: (flight['CRSDepTime'][:-2], 1)).reduceByKey(lambda a, b: a + b)\n",
    "flight_times_formatted = flight_times.map(lambda flight: ('0' + flight[0], flight[1]) if flight[0] == '' else flight)\n",
    "flights_sorted = flight_times_formatted.sortBy(lambda flight: int(flight[0]))\n",
    "flights_sorted.collect()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_practical",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
